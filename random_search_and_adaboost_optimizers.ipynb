{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "random search and adaboost optimizers.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rr4CJyCOBDH"
      },
      "source": [
        "# importing important libraries to use\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import BatchNormalization, Conv2D, Dropout, Dense \n",
        "#from keras.layers import GaussianNoise, GlobalAveragePooling2D, MaxPool2D\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, Flatten\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "from keras import losses\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#from keras.utils import multi_gpu_model\n",
        "\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, GlobalAveragePooling2D, Dense, Dropout\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "from statistics import mean, stdev\n",
        "import sklearn\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#from kfolds import build2LevelDataframe, makeSubdirFolds\n",
        "#from kfolds import kfoldValidateModelPhotos, kfoldValidateModelSessions\n",
        "\n",
        "\n",
        "\n",
        "########################################################################\n",
        "#\n",
        "# Downloads the MNIST data-set for recognizing hand-written digits.\n",
        "#\n",
        "# Implemented in Python 3.6\n",
        "#\n",
        "# Usage:\n",
        "# 1) Create a new object instance: data = MNIST(data_dir=\"data/MNIST/\")\n",
        "#    This automatically downloads the files to the given dir.\n",
        "# 2) Use the training-set as data.x_train, data.y_train and data.y_train_cls\n",
        "# 3) Get random batches of training data using data.random_batch()\n",
        "# 4) Use the test-set as data.x_test, data.y_test and data.y_test_cls\n",
        "#\n",
        "########################################################################\n",
        "#\n",
        "# This file is part of the TensorFlow Tutorials available at:\n",
        "#\n",
        "# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
        "#\n",
        "# Published under the MIT License. See the file LICENSE for details.\n",
        "#\n",
        "# Copyright 2016-18 by Magnus Erik Hvass Pedersen\n",
        "#\n",
        "########################################################################\n",
        "\n",
        "import numpy as np\n",
        "import gzip\n",
        "import os\n",
        "import sys\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import zipfile\n",
        "#from sklearn.preprocessing import OneHotEncoder\n",
        "#from dataset import one_hot_encoded\n",
        "#from download import download\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYWAED-2abAH"
      },
      "source": [
        "# Base URL for downloading the data-files from the internet.\n",
        "base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
        "\n",
        "# Filenames for the data-set.\n",
        "filename_x_train = \"train-images-idx3-ubyte.gz\"\n",
        "filename_y_train = \"train-labels-idx1-ubyte.gz\"\n",
        "filename_x_test = \"t10k-images-idx3-ubyte.gz\"\n",
        "filename_y_test = \"t10k-labels-idx1-ubyte.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr00VUC4aj9a"
      },
      "source": [
        "def _print_download_progress(count, block_size, total_size):\n",
        "    \"\"\"\n",
        "    Function used for printing the download progress.\n",
        "    Used as a call-back function in maybe_download_and_extract().\n",
        "    \"\"\"\n",
        "\n",
        "    # Percentage completion.\n",
        "    pct_complete = float(count * block_size) / total_size\n",
        "\n",
        "    # Limit it because rounding errors may cause it to exceed 100%.\n",
        "    pct_complete = min(1.0, pct_complete)\n",
        "\n",
        "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
        "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
        "\n",
        "    # Print it.\n",
        "    sys.stdout.write(msg)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "########################################################################\n",
        "def download(base_url, filename, download_dir):\n",
        "    \"\"\"\n",
        "    Download the given file if it does not already exist in the download_dir.\n",
        "    :param base_url: The internet URL without the filename.\n",
        "    :param filename: The filename that will be added to the base_url.\n",
        "    :param download_dir: Local directory for storing the file.\n",
        "    :return: Nothing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Path for local file.\n",
        "    save_path = os.path.join(download_dir, filename)\n",
        "\n",
        "    # Check if the file already exists, otherwise we need to download it now.\n",
        "    if not os.path.exists(save_path):\n",
        "        # Check if the download directory exists, otherwise create it.\n",
        "        if not os.path.exists(download_dir):\n",
        "            os.makedirs(download_dir)\n",
        "\n",
        "        print(\"Downloading\", filename, \"...\")\n",
        "\n",
        "        # Download the file from the internet.\n",
        "        url = base_url + filename\n",
        "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
        "                                                  filename=save_path,\n",
        "                                                  reporthook=_print_download_progress)\n",
        "\n",
        "        print(\" Done!\")\n",
        "\n",
        "def one_hot_encoded(class_numbers, num_classes=None):\n",
        "    \"\"\"\n",
        "    Generate the One-Hot encoded class-labels from an array of integers.\n",
        "    For example, if class_number=2 and num_classes=4 then\n",
        "    the one-hot encoded label is the float array: [0. 0. 1. 0.]\n",
        "    :param class_numbers:\n",
        "        Array of integers with class-numbers.\n",
        "        Assume the integers are from zero to num_classes-1 inclusive.\n",
        "    :param num_classes:\n",
        "        Number of classes. If None then use max(class_numbers)+1.\n",
        "    :return:\n",
        "        2-dim array of shape: [len(class_numbers), num_classes]\n",
        "    \"\"\"\n",
        "\n",
        "    # Find the number of classes if None is provided.\n",
        "    # Assumes the lowest class-number is zero.\n",
        "    if num_classes is None:\n",
        "        num_classes = np.max(class_numbers) + 1\n",
        "\n",
        "    return np.eye(num_classes, dtype=float)[class_numbers]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ1Gi08jaz9g"
      },
      "source": [
        "class MNIST:\n",
        "    \"\"\"\n",
        "    The MNIST data-set for recognizing hand-written digits.\n",
        "    This automatically downloads the data-files if they do\n",
        "    not already exist in the local data_dir.\n",
        "    Note: Pixel-values are floats between 0.0 and 1.0.\n",
        "    \"\"\"\n",
        "\n",
        "    # The images are 28 pixels in each dimension.\n",
        "    img_size = 28\n",
        "\n",
        "    # The images are stored in one-dimensional arrays of this length.\n",
        "    img_size_flat = img_size * img_size\n",
        "\n",
        "    # Tuple with height and width of images used to reshape arrays.\n",
        "    img_shape = (img_size, img_size)\n",
        "\n",
        "    # Number of colour channels for the images: 1 channel for gray-scale.\n",
        "    num_channels = 1\n",
        "\n",
        "    # Tuple with height, width and depth used to reshape arrays.\n",
        "    # This is used for reshaping in Keras.\n",
        "    img_shape_full = (img_size, img_size, num_channels)\n",
        "\n",
        "    # Number of classes, one class for each of 10 digits.\n",
        "    num_classes = 10\n",
        "\n",
        "    def __init__(self, data_dir=\"data/MNIST/\"):\n",
        "        \"\"\"\n",
        "        Load the MNIST data-set. Automatically downloads the files\n",
        "        if they do not already exist locally.\n",
        "        :param data_dir: Base-directory for downloading files.\n",
        "        \"\"\"\n",
        "\n",
        "        # Copy args to self.\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        # Number of images in each sub-set.\n",
        "        self.num_train = 55000\n",
        "        self.num_val = 5000\n",
        "        self.num_test = 10000\n",
        "\n",
        "        # Download / load the training-set.\n",
        "        x_train = self._load_images(filename=filename_x_train)\n",
        "        y_train_cls = self._load_cls(filename=filename_y_train)\n",
        "\n",
        "        # Split the training-set into train / validation.\n",
        "        # Pixel-values are converted from ints between 0 and 255\n",
        "        # to floats between 0.0 and 1.0.\n",
        "        self.x_train = x_train[0:self.num_train] / 255.0\n",
        "        self.x_val = x_train[self.num_train:] / 255.0\n",
        "        self.y_train_cls = y_train_cls[0:self.num_train]\n",
        "        self.y_val_cls = y_train_cls[self.num_train:]\n",
        "\n",
        "        # Download / load the test-set.\n",
        "        self.x_test = self._load_images(filename=filename_x_test) / 255.0\n",
        "        self.y_test_cls = self._load_cls(filename=filename_y_test)\n",
        "\n",
        "        # Convert the class-numbers from bytes to ints as that is needed\n",
        "        # some places in TensorFlow.\n",
        "        self.y_train_cls = self.y_train_cls.astype(np.int)\n",
        "        self.y_val_cls = self.y_val_cls.astype(np.int)\n",
        "        self.y_test_cls = self.y_test_cls.astype(np.int)\n",
        "\n",
        "        # Convert the integer class-numbers into one-hot encoded arrays.\n",
        "        self.y_train = one_hot_encoded(class_numbers=self.y_train_cls,\n",
        "                                       num_classes=self.num_classes)\n",
        "        self.y_val = one_hot_encoded(class_numbers=self.y_val_cls,\n",
        "                                     num_classes=self.num_classes)\n",
        "        self.y_test = one_hot_encoded(class_numbers=self.y_test_cls,\n",
        "                                      num_classes=self.num_classes)\n",
        "        #self.y_train = OneHotEncoder(class_numbers=self.y_train_cls,\n",
        "        #                               num_classes=self.num_classes)\n",
        "        #self.y_val = OneHotEncoder(class_numbers=self.y_val_cls,\n",
        "        #                             num_classes=self.num_classes)\n",
        "        #self.y_test = OneHotEncoder(class_numbers=self.y_test_cls,\n",
        "        #                              num_classes=self.num_classes)\n",
        "\n",
        "    def _load_data(self, filename, offset):\n",
        "        \"\"\"\n",
        "        Load the data in the given file. Automatically downloads the file\n",
        "        if it does not already exist in the data_dir.\n",
        "        :param filename: Name of the data-file.\n",
        "        :param offset: Start offset in bytes when reading the data-file.\n",
        "        :return: The data as a numpy array.\n",
        "        \"\"\"\n",
        "\n",
        "        # Download the file from the internet if it does not exist locally.\n",
        "        download(base_url=base_url, filename=filename, download_dir=self.data_dir)\n",
        "\n",
        "        # Read the data-file.\n",
        "        path = os.path.join(self.data_dir, filename)\n",
        "        with gzip.open(path, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=offset)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _load_images(self, filename):\n",
        "        \"\"\"\n",
        "        Load image-data from the given file.\n",
        "        Automatically downloads the file if it does not exist locally.\n",
        "        :param filename: Name of the data-file.\n",
        "        :return: Numpy array.\n",
        "        \"\"\"\n",
        "\n",
        "        # Read the data as one long array of bytes.\n",
        "        data = self._load_data(filename=filename, offset=16)\n",
        "\n",
        "        # Reshape to 2-dim array with shape (num_images, img_size_flat).\n",
        "        images_flat = data.reshape(-1, self.img_size_flat)\n",
        "\n",
        "        return images_flat\n",
        "\n",
        "    def _load_cls(self, filename):\n",
        "        \"\"\"\n",
        "        Load class-numbers from the given file.\n",
        "        Automatically downloads the file if it does not exist locally.\n",
        "        :param filename: Name of the data-file.\n",
        "        :return: Numpy array.\n",
        "        \"\"\"\n",
        "        return self._load_data(filename=filename, offset=8)\n",
        "\n",
        "    def random_batch(self, batch_size=32):\n",
        "        \"\"\"\n",
        "        Create a random batch of training-data.\n",
        "        :param batch_size: Number of images in the batch.\n",
        "        :return: 3 numpy arrays (x, y, y_cls)\n",
        "        \"\"\"\n",
        "\n",
        "        # Create a random index into the training-set.\n",
        "        idx = np.random.randint(low=0, high=self.num_train, size=batch_size)\n",
        "\n",
        "        # Use the index to lookup random training-data.\n",
        "        x_batch = self.x_train[idx]\n",
        "        y_batch = self.y_train[idx]\n",
        "        y_batch_cls = self.y_train_cls[idx]\n",
        "\n",
        "        return x_batch, y_batch, y_batch_cls\n",
        "\n",
        "\n",
        "########################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYD0ExVSbDme"
      },
      "source": [
        "NUM_GPU = 5\n",
        "\n",
        "if NUM_GPU != 1:\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "with strategy.scope():\n",
        "    input_layer = Input(shape = (28*28,))\n",
        "    FX = Flatten()(input_layer)\n",
        "\n",
        "    X = Dense(1024, activation = 'relu')(FX)\n",
        "    #CX = X\n",
        "    CX = concatenate([X, FX], axis = -1)\n",
        "\n",
        "    X = Dense(1024, activation = 'relu')(CX)\n",
        "    #CX = X\n",
        "    CX = concatenate([X, FX], axis = -1)\n",
        "\n",
        "    X = Dense(1024, activation = 'relu')(CX)\n",
        "    #CX = X\n",
        "    CX = concatenate([X, FX], axis = -1)\n",
        "\n",
        "    X = Dense(10, activation = 'softmax')(CX)\n",
        "    \n",
        "    model = Model(input_layer, X, name = 'Cascade')\n",
        "\n",
        "    model.summary()\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(lr=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "\n",
        "data = MNIST(data_dir=\"/media/data2/MNIST\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikyEwazebWOu"
      },
      "source": [
        "x_train = tf.keras.utils.normalize(data.x_train, axis=1)\n",
        "x_test = tf.keras.utils.normalize(data.x_test, axis=1)\n",
        "\n",
        "val_data = (data.x_val, data.y_val)\n",
        "\n",
        "hist = model.fit(x_train, data.y_train, epochs=4, batch_size=512, validation_data=val_data) \n",
        "\n",
        "accuracy = hist.history['val_accuracy'][-1]\n",
        "print()\n",
        "print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRYkAeI9M0Qt"
      },
      "source": [
        "1.ADAPTIVE BOOSTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAJz757HEGGt"
      },
      "source": [
        "# ADAPTIVE BOOSTING\n",
        "# using Adaptive boosting to optimise the structure and parameters of ANN\n",
        "# using sklearn adaboost classifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# adaboost instance\n",
        "adaboostclf = AdaBoostClassifier(model, n_estimators=5000, algorithm=\"SAMME\", learning_rate=0.2, random_state=42)\n",
        "\n",
        "# training the model\n",
        "print(\"model under training.....\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DIpXOwatliU"
      },
      "source": [
        "# using a own version of mnist dataset with preffered tweaks\n",
        "# errors experienced using the first version of mnist dataset, one used on the model above\n",
        "\n",
        "def sort_by_target(mnist):\n",
        "    reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1]\n",
        "    reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1]\n",
        "    mnist.data[:60000] = mnist.data[reorder_train]\n",
        "    mnist.target[:60000] = mnist.target[reorder_train]\n",
        "    mnist.data[60000:] = mnist.data[reorder_test + 60000]\n",
        "    mnist.target[60000:] = mnist.target[reorder_test + 60000]\n",
        "\n",
        "try:\n",
        "    from sklearn.datasets import fetch_openml\n",
        "    mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
        "    mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings\n",
        "    sort_by_target(mnist) # fetch_openml() returns an unsorted dataset\n",
        "except ImportError:\n",
        "    from sklearn.datasets import fetch_mldata\n",
        "    mnist = fetch_mldata('MNIST original')\n",
        "mnist[\"data\"], mnist[\"target\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuPB539fuJcu"
      },
      "source": [
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz0T0RJBJGQy"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-oN2_IQJdY1"
      },
      "source": [
        "X_train, X_test, y_train, y_test = X[:30000], X[30000:], y[:30000], y[30000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CqPlOocJ25A"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsjiL0zuJ-nG"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUjjbK0I2CoL"
      },
      "source": [
        "# using sgd classifier instead of the model above because of the errors encountered\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "sgd_clf = SGDClassifier(random_state=42)\n",
        "sgd_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wqbxotc3g2B"
      },
      "source": [
        "# using Adaptive boosting to optimise the structure and parameters of ANN\n",
        "# using sklearn adaboost classifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# adaboost instance\n",
        "adaboostclf = AdaBoostClassifier(sgd_clf, n_estimators=5000, algorithm=\"SAMME\", learning_rate=0.2, random_state=42)\n",
        "\n",
        "# training the model\n",
        "print(\"model under training.....\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTH8v9V0fcY0"
      },
      "source": [
        "# fitting adaboostclf to the dataset\n",
        "# the training time is long, couple minutes\n",
        "search = adaboostclf.fit(X_train,y_train)\n",
        "# summarize result\n",
        "print('Best Score: %s' % search.best_score_)\n",
        "print('Best Hyperparameters: %s' % search.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBBCaw7BVPdG"
      },
      "source": [
        "y_pred = adaboostclf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOn1C356PeAY"
      },
      "source": [
        "adaboostclf.score(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y60jGhbTM9Kl"
      },
      "source": [
        "2.**RANDOM** SEARCH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr-BhI6v1P5v"
      },
      "source": [
        "# RANDOM SEARCH\n",
        "# using random search to optimise the structure and paramers of ANN\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from scipy.stats import uniform\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "distributions = dict(C=uniform(loc=0, scale=4),penalty=['l2', 'l1'])\n",
        "\n",
        "# define search space\n",
        "space = dict()\n",
        "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
        "space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\n",
        "space['C'] = loguniform(1e-5, 100)\n",
        "# define evaluation\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "clf = RandomizedSearchCV(sgd_clf, space,n_iter=500,scoring='accuracy',n_jobs=-1,cv=cv, random_state=0)\n",
        "search = clf.fit(X_train, y_train)\n",
        "# summarize result\n",
        "print('Best Score: %s' % search.best_score_)\n",
        "print('Best Hyperparameters: %s' % search.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}